{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda float32\n"
     ]
    }
   ],
   "source": [
    "# https://habr.com/ru/post/347564/.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "os.environ['THEANO_FLAGS'] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "import theano\n",
    "print(theano.config.device, theano.config.floatX)\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "#from google.colab import files # для импорта данных в google colab\n",
    "#from google.colab import drive # для импорта данных в google colab из google drive\n",
    "import zipfile # для работы с архивами \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as im\n",
    "import time\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import densenet\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model  \n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "class_names = ['1', '2', '3', '4', '5']\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 71, 71\n",
    "nb_train_samples = 26640\n",
    "nb_validation_samples = 12630\n",
    "epochs = 12\n",
    "batch_size = 8\n",
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/datasets/DTLD/DTLD_crop/new_train/position\"    # дальше там папки, а не сами фотографии!!!!\n",
    "validation_data_dir = \"/datasets/DTLD/DTLD_crop/new_test/position\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #fill_mode = 'constant',\n",
    "    #cval = 1,\n",
    "    rotation_range = 5,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40673 images belonging to 5 classes.\n",
      "Found 9799 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_Xception():\n",
    "  base_model = xception.Xception(include_top=True, \n",
    "                                      weights = None,\n",
    "                                      input_tensor = None,\n",
    "                                      input_shape=(img_width, img_height, 3),\n",
    "                                      pooling=None,\n",
    "                                      classes = 5)\n",
    "  \n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "      \n",
    "  model = Model(inputs = base_model.input, outputs = base_model.output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_recall(y_true, y_pred):\n",
    "    recall = list()\n",
    "    for i in range(5):\n",
    "        a = 0\n",
    "        b = 0\n",
    "        print(type(y_true))\n",
    "        for j in range(y_true.size()[0]):\n",
    "            if y_true[j] == i and y_pred == i:\n",
    "                a += 1\n",
    "            if y_true[j] == i and y_pred != i:\n",
    "                b += 1\n",
    "    recall.append([a/(a+b)])\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model_Xception()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4) # остановка обучения, если loss на валидационном множесте улучшается менее чем на 10^-4\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\n",
    "filepath=\"weights-position-improvement-71-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "check = ModelCheckpoint(filepath, monitor = \"val_acc\", save_best_only = False) # сохранение лучшей (с наибольшим acc на валидационном множестве) сети\n",
    "callbacks_list = [early_stop, reduce_lr , check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/12\n",
      "3330/3330 [==============================] - 10564s 3s/step - loss: 0.4953 - acc: 0.8726 - mean_squared_error: 0.0390 - val_loss: 0.2040 - val_acc: 0.9449 - val_mean_squared_error: 0.0178\n",
      "Epoch 2/12\n",
      "3330/3330 [==============================] - 10216s 3s/step - loss: 0.2921 - acc: 0.9115 - mean_squared_error: 0.0258 - val_loss: 0.1673 - val_acc: 0.9563 - val_mean_squared_error: 0.0139\n",
      "Epoch 3/12\n",
      "3330/3330 [==============================] - 7808s 2s/step - loss: 0.2202 - acc: 0.9317 - mean_squared_error: 0.0200 - val_loss: 0.1402 - val_acc: 0.9567 - val_mean_squared_error: 0.0131\n",
      "Epoch 4/12\n",
      "3330/3330 [==============================] - 7288s 2s/step - loss: 0.1926 - acc: 0.9395 - mean_squared_error: 0.0177 - val_loss: 0.1164 - val_acc: 0.9635 - val_mean_squared_error: 0.0110\n",
      "Epoch 5/12\n",
      "3330/3330 [==============================] - 6957s 2s/step - loss: 0.1766 - acc: 0.9440 - mean_squared_error: 0.0164 - val_loss: 0.1158 - val_acc: 0.9664 - val_mean_squared_error: 0.0104\n",
      "Epoch 6/12\n",
      "3330/3330 [==============================] - 7740s 2s/step - loss: 0.1590 - acc: 0.9485 - mean_squared_error: 0.0149 - val_loss: 0.1099 - val_acc: 0.9648 - val_mean_squared_error: 0.0104\n",
      "Epoch 7/12\n",
      "3330/3330 [==============================] - 8332s 3s/step - loss: 0.1439 - acc: 0.9554 - mean_squared_error: 0.0133 - val_loss: 0.1113 - val_acc: 0.9655 - val_mean_squared_error: 0.0098\n",
      "Epoch 8/12\n",
      "3330/3330 [==============================] - 8338s 3s/step - loss: 0.1370 - acc: 0.9578 - mean_squared_error: 0.0125 - val_loss: 0.0846 - val_acc: 0.9767 - val_mean_squared_error: 0.0074\n",
      "Epoch 9/12\n",
      "3330/3330 [==============================] - 8401s 3s/step - loss: 0.1249 - acc: 0.9611 - mean_squared_error: 0.0117 - val_loss: 0.0745 - val_acc: 0.9790 - val_mean_squared_error: 0.0066\n",
      "Epoch 10/12\n",
      "3330/3330 [==============================] - 8910s 3s/step - loss: 0.1159 - acc: 0.9639 - mean_squared_error: 0.0108 - val_loss: 0.0808 - val_acc: 0.9761 - val_mean_squared_error: 0.0074\n",
      "Epoch 11/12\n",
      "3330/3330 [==============================] - 8622s 3s/step - loss: 0.1067 - acc: 0.9673 - mean_squared_error: 0.0099 - val_loss: 0.0669 - val_acc: 0.9791 - val_mean_squared_error: 0.0062\n",
      "Epoch 12/12\n",
      "1250/3330 [==========>...................] - ETA: 1:21:29 - loss: 0.1037 - acc: 0.9657 - mean_squared_error: 0.0099"
     ]
    }
   ],
   "source": [
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size,\n",
    "    callbacks = callbacks_list,\n",
    "    steps_per_epoch = nb_train_samples // batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('weights-position-improvement-12-0.98.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc', 'mse', precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225/1225 [==============================] - 1376s 1s/step - loss: 0.0488 - acc: 0.9846 - mean_squared_error: 0.0046 - precision: 0.9878 - recall: 0.9816\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate_generator(validation_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'mean_squared_error', 'precision', 'recall']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.048828357186359646, 0.9845903, 0.0046078614, 0.9878134, 0.98163265]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:open-mmlab]",
   "language": "python",
   "name": "conda-env-open-mmlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
