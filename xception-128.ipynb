{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'In file included from /tmp/try_flags_zow2a2j9.c:4:0:\\n/usr/include/cudnn.h:63:10: fatal error: driver_types.h: No such file or directory\\n #include \"driver_types.h\"\\n          ^~~~~~~~~~~~~~~~\\ncompilation terminated.\\n'\n",
      "Mapped name None to device cuda: Tesla V100-SXM2-32GB (0000:04:00.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://habr.com/ru/post/347564/.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "os.environ['THEANO_FLAGS'] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "import theano\n",
    "print(theano.config.device, theano.config.floatX)\n",
    "\n",
    "import keras\n",
    "import keras_metrics\n",
    "import sys\n",
    "#from google.colab import files # для импорта данных в google colab\n",
    "#from google.colab import drive # для импорта данных в google colab из google drive\n",
    "import zipfile # для работы с архивами \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.applications import xception\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as im\n",
    "import time\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import densenet\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "class_names = ['0', '1', '2', '3', '4', '5']\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 128, 128\n",
    "nb_train_samples = 26640\n",
    "nb_validation_samples = 12630\n",
    "epochs = 12\n",
    "batch_size = 8\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/datasets/DTLD/DTLD_crop/new_train/colours\"    # дальше там папки, а не сами фотографии!!!!\n",
    "validation_data_dir = \"/datasets/DTLD/DTLD_crop/new_test/colours\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #fill_mode = 'constant',\n",
    "    #cval = 1,\n",
    "    rotation_range = 5,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38301 images belonging to 6 classes.\n",
      "Found 9111 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_Xception():\n",
    "  base_model = xception.Xception(include_top=True, \n",
    "                                      weights = None,\n",
    "                                      input_tensor = None,\n",
    "                                      input_shape=(img_width, img_height, 3),\n",
    "                                      pooling=None,\n",
    "                                      classes = 6)\n",
    "  \n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "      \n",
    "  model = Model(inputs = base_model.input, outputs = base_model.output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model_Xception()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4) # остановка обучения, если loss на валидационном множесте улучшается менее чем на 10^-4\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\n",
    "filepath=\"weights-improvement-128-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "check = ModelCheckpoint(filepath, monitor = \"val_acc\", save_best_only = False) # сохранение лучшей (с наибольшим acc на валидационном множестве) сети\n",
    "callbacks_list = [early_stop, reduce_lr , check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z_andrei/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/12\n",
      "3330/3330 [==============================] - 12128s 4s/step - loss: 0.4856 - acc: 0.8567 - mean_squared_error: 0.0355 - val_loss: 0.3191 - val_acc: 0.9055 - val_mean_squared_error: 0.0227\n",
      "Epoch 2/12\n",
      "3330/3330 [==============================] - 11934s 4s/step - loss: 0.3017 - acc: 0.9121 - mean_squared_error: 0.0219 - val_loss: 0.1983 - val_acc: 0.9397 - val_mean_squared_error: 0.0146\n",
      "Epoch 3/12\n",
      "3330/3330 [==============================] - 9030s 3s/step - loss: 0.2320 - acc: 0.9331 - mean_squared_error: 0.0172 - val_loss: 0.2081 - val_acc: 0.9369 - val_mean_squared_error: 0.0151\n",
      "Epoch 4/12\n",
      "3330/3330 [==============================] - 8653s 3s/step - loss: 0.2094 - acc: 0.9396 - mean_squared_error: 0.0156 - val_loss: 0.1813 - val_acc: 0.9448 - val_mean_squared_error: 0.0136\n",
      "Epoch 5/12\n",
      "3330/3330 [==============================] - 8670s 3s/step - loss: 0.1933 - acc: 0.9462 - mean_squared_error: 0.0142 - val_loss: 0.1887 - val_acc: 0.9419 - val_mean_squared_error: 0.0143\n",
      "Epoch 6/12\n",
      "3330/3330 [==============================] - 8612s 3s/step - loss: 0.1715 - acc: 0.9508 - mean_squared_error: 0.0128 - val_loss: 0.1852 - val_acc: 0.9529 - val_mean_squared_error: 0.0126\n",
      "Epoch 7/12\n",
      "3330/3330 [==============================] - 8587s 3s/step - loss: 0.1587 - acc: 0.9554 - mean_squared_error: 0.0116 - val_loss: 0.1600 - val_acc: 0.9548 - val_mean_squared_error: 0.0115\n",
      "Epoch 8/12\n",
      "3330/3330 [==============================] - 8644s 3s/step - loss: 0.1559 - acc: 0.9570 - mean_squared_error: 0.0113 - val_loss: 0.1435 - val_acc: 0.9589 - val_mean_squared_error: 0.0104\n",
      "Epoch 9/12\n",
      "3330/3330 [==============================] - 10938s 3s/step - loss: 0.1420 - acc: 0.9604 - mean_squared_error: 0.0104 - val_loss: 0.1610 - val_acc: 0.9560 - val_mean_squared_error: 0.0113\n",
      "Epoch 10/12\n",
      "1194/3330 [=========>....................] - ETA: 2:04:45 - loss: 0.1369 - acc: 0.9611 - mean_squared_error: 0.0102"
     ]
    }
   ],
   "source": [
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size,\n",
    "    callbacks = callbacks_list,\n",
    "    steps_per_epoch = nb_train_samples // batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.EarlyStopping at 0x7f0c7c34aa90>,\n",
       " <keras.callbacks.ReduceLROnPlateau at 0x7f0c7c34aa20>,\n",
       " <keras.callbacks.ModelCheckpoint at 0x7f0c7c34aac8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('weights-improvement-12-0.97.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc', 'mse', precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139/1139 [==============================] - 1381s 1s/step - loss: 0.1343 - acc: 0.9663 - mean_squared_error: 0.0088 - precision: 0.9691 - recall: 0.9640\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate_generator(validation_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'mean_squared_error', 'precision', 'recall']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1342851871318884, 0.9663045, 0.00879281, 0.9690518, 0.9639879]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
